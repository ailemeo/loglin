{
    "lesson_order" : [
	1,
	2,
	3,
	4,
	5,
	6,
	7,
	8,
	9,
	10,
	11,
	12,
	13,
	14,
	15,
	16,
	"NP_number",
	17,
	18
    ],
    "tooltips" : {
	"attr" : "title", 
	"list": {
	    ".num_tokens_context_div" : "The total number of observations in this context (changing it will rescale all the gray counts).",
	    ".observed_count" : "",
	    "#solve_button" : "Find best feature weights (sliders) given the observed counts (gray outlines). Regularization alters the definition of \"best.\"",
	    "#new_challenge" : "Sample a new training dataset from a <i>new</i> probability distribution.  From the new gray counts, you can then try to estimate the secret weights that define the new distribution.",
	    "#new_counts" : "Sample a new training dataset from the <i>same</i> probability distribution.  Small datasets can differ dramatically, leading you to different estimates of the weights.  Regularization tries to reduce this variance.",
	    "#cheat_button": "Reveal the weights of the true distribution, and its regularized log-likelihood.",
	    "#regularization_constant" : "The strength of the regularizer.",
	    "#gradient_step" : "Change how strongly you follow the current gradient.",
	    ".handle" : ""
	}
    }
}
