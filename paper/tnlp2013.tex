\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage[lowtilde]{url}
%\usepackage[colorlinks]{hyperref}  % messes up our bib format, and doesn't work with Author (Year) citations
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate,paralist,enumitem}
\usepackage{verbatim}
\usepackage{mathtools}

\usepackage{array}

\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

\DeclareMathOperator*{\argmax}{arg\,max}

%\usepackage{authordate1-4}
\usepackage{multirow}

\usepackage{color,soul}
\usepackage{transparent}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\newcommand{\Note}[1]{}
%\renewcommand{\Note}[1]{\hl{[#1]}}  % comment this out to remove notes
\newcommand{\FIXME}{\Note{FIXME}}
\newcommand{\NoteSigned}[3]{{\sethlcolor{#2}\Note{#1: #3}}}
\newcommand{\RemoveFF}[1]{\NoteSigned{Remove (FF)}{Crimson}{#1}}
\newcommand{\NoteFF}[1]{\NoteSigned{FF}{LightBlue}{#1}}
\newcommand{\NoteJE}[1]{\NoteSigned{JE}{LightGreen}{#1}}
\newcommand{\Commented}[1]{#1}

\newcommand{\empirical}[0]{\ensuremath{\tilde{p}}}
\newcommand{\Data}[0]{\ensuremath{\mathcal{D}}}

\newcommand{\WhereToFind}[0]{\url{http://cs.jhu.edu/~jason/tutorials/loglin/}}
\newcommand{\les}[1]{\textbf{#1}}

%%%%%%%%%%%%%%%%%5
%CHANGE THIS!!!
\newcommand{\NumLessons}[0]{18}% \NoteFF{change \#: 18}}

\setlength\titlebox{6.5cm}    % Expanding the titlebox


\title{A Virtual Manipulative for Learning Log-Linear Models}


\author{
Francis Ferraro \and Jason Eisner\\
Department of Computer Science\\
Johns Hopkins University\\
Baltimore, MD, USA\\
{\tt \{ferraro, jason\}@cs.jhu.edu}
}
  
\date{}

\begin{document}

\maketitle

\begin{abstract}
  We present an open-source virtual manipulative for conditional
  log-linear models. This web-based interactive visualization lets
  the user tune the probabilities of various shapes---which grow and
  shrink accordingly---by dragging sliders that correspond to feature
  weights.  The visualization displays a regularized training
  objective; it supports gradient ascent by optionally displaying
  gradients on the sliders and providing ``Step'' and ``Solve''
  buttons.  The user can sample parameters and datasets of
  different sizes and compare their own parameters to the truth.  Our
  website, \WhereToFind{}, guides the user through a series of
  interactive lessons and provides auxiliary readings, explanations, 
practice problems and resources.
\end{abstract}

\section{Introduction}\label{sec:intro}
We argue that if one is going to teach only a single machine learning
technique in a computational linguistics course, it should be {\em
  conditional log-linear modeling}.  Such models are pervasive in
natural language processing.  They have the form
\begin{equation}\label{eqn:loglin}
p_{\vec{\theta}}(y \mid x) \propto \exp{\left(\vec{\theta} \cdot \vec{f}\left(x,y\right)\right)},
\end{equation}
where $\vec{f}$ extracts a feature vector from context $x$ and
outcome $y \in \mathcal{Y}(x)$.  The set of possible
outcomes $\mathcal{Y}(x)$ might depend on the context
$x$.\footnote{\label{fn:logistic}The model is equivalent to logistic regression
  when $y$ is a binary variable, that is, when $\mathcal{Y}(x)=\{0,1\}$.}

We then present an interactive web visualization that guides students
through playing with log-linear models and their estimation. This open-source 
tool, available at \WhereToFind{}, is intended to develop intuitions, so that basic
log-linear models can be then taken for granted in future lectures.  It can be used near
the start of a course, perhaps after introducing probability notation
and $n$-gram models.

\begin{figure*}
\centering
%\includegraphics[scale=.49]{images/lesson1-051313-intro-zoom-instmore.PNG}
\includegraphics[scale=.47]{images/lesson1-060713-intro-zoom-with18.PNG}
\caption{The first lesson; the lower half is larger on the actual
  application.}
\label{fig:lesson1}
\end{figure*}

We used the tool in our Natural Language Processing (NLP) class and received
very positive feedback.  Students were excited by it, with some saying
the tool helped develop their ``physical intuition'' for log-linear
models.
%\NoteJE{they did? who?}  
%\NoteFF{Students, either through email, comments in homework assignments 
%or one-on-one discussions. I can send names via email if you like.}
Other test users with no 
% statistical, computational, or
technical background also enjoyed working
through the introductory lessons and found that they began to understand 
the model.

The app includes \NumLessons{} ready-to-use lessons for individual or
small-group study or classroom use.  Each lesson, e.g. Figure
\ref{fig:lesson1}, guides the student to fit a probability model $p_{\vec{\theta}}(y \mid
x)$ over some collection ${\cal Y}$ of shapes, words, or other images
such as parse trees.  Each lesson is peppered with questions; students
can be asked to answer some of these questions in
writing.\footnote{There are approximately 6 questions per lesson. We
  found that answering {\em all} the questions took our students about
  2300 words, or just under 23 words per question, which was probably
  both unreasonable and unnecessary.}  Ambitious instructors can add new
lessons or edit existing ones by writing configuration files (see
section~\ref{sec:tailoring}). This is useful for emphasizing specific
concepts or applications.
% ; instructors may also use it to prepare students for any upcoming assignments.  
Section \ref{sec:history} provides some history and applications of log-linear
modeling, as well as assignment ideas.

\section{Why Teach With Log-Linear Models?}\label{sec:whyloglin}

Log-linear models are very handy in NLP.  They can be used {\em throughout} 
a course, when one needs 
\begin{itemize}
\item a global classifier for an applied task, such as detecting
  sentiment, topic, spam, or gender;
\item a local classifier for structure annotation,
  such as tags or segment boundaries;
\item a local classifier to be applied repeatedly in sequential decision-making;
\item a local conditional probability within some generative process, such
  as an $n$-gram model, HMM, PCFG, probabilistic FSA or FST, noisy-channel MT model,
  or Bayes net;
\item a global structured prediction method.  Here $y$ is a complete
  structured object such as a tagging, segmentation, parse, alignment, or translation.  Then $p(y
  \mid x)$ is a Markov random field or a conditional random field,
  depending on whether $x$ is empty or not.
\end{itemize}  

Log-linear models over discrete variables are also sufficiently
expressive for an NLP course.  Students may experiment freely with
adding their own creative model \textit{features} that refer to 
salient \textit{attributes} or \textit{properties} of the data, 
since the probability \eqref{eqn:loglin} may consider any number of
informative features of the $(x,y)$ pair.

How about training?  Estimation of the parameter weights
$\vec{\theta}$ from a set of fully observed $(x,y)$ pairs is simply a
convex optimization problem.  Maximizing the regularized conditional
log-likelihood
\begin{equation}\label{eqn:reg_ll}
% \argmax_{\vec{\theta}} \sum_i \log p_{\vec{\theta}}(y_i \mid x_i) - C\cdot R\left(\vec{\theta}\right)
  F(\vec{\theta}) = \left( \sum_{i=1}^N \log{p_{\vec{\theta}}\left(y_i\ \mid\ x_i\right)} \right) - C \cdot R(\vec{\theta})
\end{equation}
is a simple, uniform training principle that can be used throughout
the course.  The scaled regularizer $C\cdot R(\vec{\theta})$
prevents overfitting on sparse features.
This is arguably more straightforward than the traditional NLP
smoothing methods for estimating probabilities from sparse data
\cite{chen-goodman-1996}, which require applying various {\em ad hoc}
formulas to counts, and which do not generalize well to settings where
there is not a natural sequence of backoff models.  There exist 
fast and usable tools that students can use to train their log-linear
models, including, among others, MegaM \cite{daume04cg-bfgs}, 
and NLTK \cite{bird2009natural}.\footnote{\label{fn:bigY}A caveat is that generic
  log-linear training tools will {\em iterate} over the set ${\cal
    Y}(x)$ in order to maximize
  \eqref{eqn:loglin} and to compute the constant of proportionality
  in \eqref{eqn:loglin} and the gradient of
  \eqref{eqn:reg_ll}.  This is impractical when ${\cal Y}(x)$ is large, as in
  language modeling or structured prediction.  See Section \ref{sec:history}.
%One will do better to
% either restrict ${\cal Y}(x)$ before training
%\cite{johnson-et-al-1999}, or else use techniques such as dynamic
%programming or sampling, e.g., 
%\cite{lafferty-mccallum-pereira-2001} and \cite{rosenfeld-chen-zhu-2001}. 
%These often exploit special
%structure in the feature set; students will have to use other tools or 
%write their own code.\label{fn:compute_partition_fn}
}

Formally, log-linear models are a good gateway to a more general
understanding of undirected graphical models and the exponential
family, including globally normalized joint or conditional
distributions over trees and sequences.

One reason that log-linear models are both versatile and pedagogically
useful is that they do not just make predictions, but explicitly 
model {\em probabilities}.  These can be 
\begin{itemize}
\item combined with other probabilities using the usual rules of probability;
\item marginalized at test time to obtain the probability that the outcome 
  $y$ has a particular property (e.g., one can sum over alignments);
\item marginalized at training time in the case of incomplete data $y$
  (e.g., the training data may not include alignments);
\item used to choose among possible decisions by computing their 
  expected loss (risk).
\end{itemize}
The training procedure also takes a probabilistic view.  Equation
\eqref{eqn:reg_ll} helps illustrate important statistical principles
such as maximum likelihood,\footnote{\label{fn:dual}Historically, this
  objective has been regarded as the optimization dual of a maximum
  entropy problem \cite{berger-dellapietra-dellapietra-1996},
  motivating the log-linear form of \eqref{eqn:reg_ll}.
  We have considered adding a maximum entropy view 
  to our manipulative.} regularization (the bias-variance
tradeoff), and cross-validation, as well as optimization principles
such as gradient ascent.

Log-linear models also provide natural extensions of commonly taught
NLP methods.  For example, under a probabilistic context-free
grammar (PCFG),\footnote{Likewise for Markov or hidden Markov models.}  $p(\text{parse tree}\mid\text{sentence})$ is proportional to
a product of rule {\em probabilities}.  Simply replacing each rule
probability with an arbitrary non-negative {\em potential}---an
exponentiated weight, or sum of weights of features of that
rule---gives an instance of \eqref{eqn:loglin}.  The same parsing
algorithms still apply without modification, as does the same
inside-outside approach to computing the posterior expectation of rule counts and 
feature counts.  Immediate variants include CRF CFGs
\cite{finkel2008efficient}, in which the rule features become
position-dependent and sentence-dependent, and log-linear PCFGs
\cite{bergkirkpatrick-et-al-2010}, in which the feature-rich rule
potentials are locally renormalized into rule probabilities via
\eqref{eqn:loglin}.

For all these reasons, we recommend log-linear models as one's
``go-to'' machine learning technique when teaching.  Other linear
classifiers, such as perceptrons and SVMs, similarly choose $y$ given
$x$ based on a linear score $\vec{f} \cdot \vec{\theta}(x,y)$---but
these scores have no probabilistic interpretation, and the procedures
for training $\vec{\theta}$ are harder to understand or to justify.
Thus, they can be taught as variants later on or in another course.
Further reading includes \cite{smith-2011}.

\section{The Teaching Challenge} \label{sec:challenges}

Unfortunately, there is a difficulty with introducing log-linear
models early in a course.  Once grasped, they seem very simple.  But
they are not so easy to grasp for a student who has not had any
experience with high-dimensional parametric functions, feature
design, or statistical estimation.  The interaction among the parameters can be
bewildering.  Log-likelihood, gradient ascent, and overfitting may also
be new ideas.

Students who lack intuitions about these models will fail to follow
subsequent lectures.  They will also have trouble with homework
projects---interpreting the weights learned by their model, and
diagnosing problems with their features or their implementation.  A
student cannot even design appropriate feature sets without
understanding how the weights of these features interact to define a
distribution.  We will discuss some of the necessary intuitions in
sections \ref{sec:aims} and \ref{sec:lessons}.

We would like equations \eqref{eqn:loglin}, \eqref{eqn:reg_ll}, and the
gradient formula to be more than just recipes.  The student should
regard them as {\em familiar objects with predictable behavior}.  Like
computer science, pedagogy proceeds by layering new ideas on top of
already-familiar abstractions.  A solid understanding of basic
log-linear models is prerequisite to 
\begin{itemize}
\item using them in NLP applications that have their own complexities, 
\item using them as component distributions within larger probability
  models or decision rules,
\item generalizing the algorithms for working with \eqref{eqn:loglin}
  and \eqref{eqn:reg_ll} to settings where one cannot easily enumerate
  ${\cal Y}$.
\end{itemize}
  
\section{(Virtual) Manipulatives}

Familiar concrete concepts have often been invoked to help develop
intuitions about abstract mathematical concepts.  Specifically within
early math education, \textit{manipulatives}---tactile objects---have
been shown to be effective hands-on teaching tools. Examples include
Cuisenaire rods for exploring arithmetic concepts like sums, ratios,
and place value, or geoboards for exploring geometric concepts like
area and perimeter.\footnote{Cuisenaire rods are color-coded blocks
  with lengths from 1 to 10.  A geoboard is a board representing the
  plane, with pegs at the integral points.  A rubber band can be
  stretched around selected pegs to define a polygon.} The key idea is
to ground and link the mathematical \textit{language} to a well-known
\textit{physical object} that can be inspected and manipulated.  For
more, see the classic and recent analyses from
\newcite{sowell1989effects} and \newcite{carbonneau2013meta}.

Research has shown concrete manipulatives to be effective, but practical widespread use of them presents certain 
problems, including procurement of necessary materials, replicability,
%\NoteJE{is this a problem
%  with using concrete manipulatives or only a problem in doing
%  research on them?} \NoteFF{using them; virtual manipulatives encourage practical, inexpensive systematic 
%deployment} 
and applicability to certain groups of students and to concepts
that have no simple physical realization. These issues
% , among others,
have spurred interest over the past two decades in
\textit{virtual manipulatives} implemented in software, including the creation of 
the National Library of Virtual Manipulatives.\footnote{\url{nlvm.usu.edu/en/nav/vlibrary.html} and 
\url{enlvm.usu.edu/ma/nav/doc/intro.jsp}} 
Both \newcite{clements1996concrete} and \newcite{moyer2002virtual} provide accessible overviews of 
virtual manipulatives in early math education. 
Virtual manipulatives give students the ability to effect changes on a
complex system and so learn its underlying 
properties \cite{moyer2002virtual}. This last point is particularly
relevant to log-linear models.

%"Clements and Sarama
%(2005) cite six categories of math that emerge through
%play: ‘‘classiﬁcation (grouping and sorting), magnitude
%(describing or comparing the size of objects), dynamics
%(putting things together and taking them apart), pattern and
%shape (identifying or creating patterns or shapes, exploring
%geometry concepts), spatial relations (describing or drawing a location or direction), and enumeration (saying
%number words, counting, recognizing a number of objects,
%or reading and writing numbers’’" \cite{linder2011early}

%\Commented{
%\NoteJE{Start by explaining the use of manipulatives in early math
%  education, perhaps including an example and a quote.  I tried to set
%  this up at the end of the previous section by talking about
%  ``familiar objects with predictable behavior.''  The bottom line is
%  that we'd like students to develop \emph{physical} intuitions about
%  these abstract models.}
%}

Members of the NLP and speech communities have previously explored manipulatives and the idea of
``learning by doing.''
\newcite{eisner-2002-tnlp} implemented HMM posterior inference and
forward-backward training on a spreadsheet, so that editing the data or initial parameters
changed the numerical computations and the resulting graphs.  
VISPER, an applied educational tool that wrapped various speech technologies, 
was targeted toward understanding the acoustics and overall recognition pipeline 
\cite{nouza1997educational}. 
\newcite{light-tnlp-2005} developed web interfaces for a number of core NLP technologies and systems, such as parsers, part-of-speech 
taggers, and finite-state transducers.  Matt Post created a Model 1 stack decoder visualization for a recent machine translation class \cite{lopez2013learning}.\footnote{\url{github.com/mjpost/stack-decoder}}
Most manipulatives/interfaces targeted at NLP have been virtual, but a notable exception is \newcite{halteren-tnlp-2002}, 
who created a (physical) board game for parsing. 

In machine learning, there is a plethora of virtual manipulatives demonstrating central concepts such 
as decision boundaries and kernel
methods.\footnote{E.g.,
  \url{http://cs.cmu.edu/~ggordon/SVMs/svm-applet.html}.
%\NoteJE{looking
%  for Caron's applet instead.  I sent an email.}
}
There are also several systems for teaching artificial intelligence: these tend to to involve controlling 
virtual robots\footnote{E.g., \url{http://www-inst.eecs.berkeley.edu/~cs188/pacman/pacman.html} and 
\url{http://www.cs.rochester.edu/trac/quagents}.}
%, or \url{http://www.cs.rochester.edu/trac/quagents}} 
or physical ones \cite{tokic2012robot}.
Overall, manipulatives for NLP and ML seem to be a successful
pedagogical direction that we hope will continue.

Next, we present our main contribution, a virtual manipulative that teaches log-linear models. We ground 
the models in simple objects such as circles and regular polygons, in order to appeal to the 
students' physical intuitions.  Later lessons can move on 
from shapes, instead using words or images from a particular
application of interest.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Log-Linear Virtual Manipulative}\label{sec:overview}

Figure \ref{fig:lesson1} shows a screenshot
of the tool, available at 
\WhereToFind{}. We encourage you to play with it as you read.

%While our primary goal was to teach log-linear models within the context of NLP, 
%we thought it myopic to outright limit users to those interested or experienced 
%in NLP. 

%Our manipulative has three primary components:
%the student interface (front-end), an easily-configurable ``middle-end''
%for the instructor, and the back-end implementation.

\subsection{Student Interface}

Successive lessons introduce various challenges or subleties.  In each lesson, the user 
% plays a model matching
% game, a feature matching game or a log-likelihood game on 
experiments with modeling some given dataset \Data{} using some
given set of $K$ features.  
Dataset: For each context $x$, the outcomes $y \in {\cal Y}(x)$ are
displayed as shapes, images or words.
Features: For each feature $f_i$, there is a slider to manipulate $\theta_i$.

%\Commented{\NoteJE{I'd
%  lead with this.  In fact, I think the text shown in Fig. 1 is
%  extremely helpful for the reader to understand what this
%  manipulative is all about, which is so far rather obscure even
%  though we're well into the paper by now.  Maybe encourage the reader
%  explicitly to read Fig. 1, and maybe enlarge it a bit (although the
%  reader can zoom in if they're reading onscreen).  In the final
%  version, you might be able to get a higher-res screenshot by zooming
%  the browser (and using an appropriately large window); this gives a
%  larger image and larger-font text, which you can shrink back again
%  in the PDF.}  }

Each shape $y$ is sized proportionately to its {\em model
probability} $p_{\vec{\theta}}(y \mid x)$ (equation~\eqref{eqn:loglin}), so it grows or shrinks as the user changes
$\vec{\theta}$.  
In contrast, the {\em empirical probability} 
\begin{equation}
\empirical\left(y\ \mid\ x\right) = \frac{c(x,y)}{c(x)} \ \ \text{\it (= ratio of counts)}
\label{eqn:empirical_distr}
\end{equation} 
is constant and is shown by a gray outline.

The size and color of $y$  indicate how $p_{\vec{\theta}}(y\mid x)$ compares to 
this empirical probability (Figure~\ref{fig:colorsize_inventory}).  
Reinforcing this, the observed count $c(x,y)$ is shown at the upper
left of $y$, while the expected count $c(x)\cdot p_{\vec{\theta}}(y\mid x)$ 
is shown at the upper right, following the same color 
scheme (Figure \ref{fig:lesson1}).  

We begin with globally normalized models (only one 
context $x$).  For example, the data in Figure \ref{fig:lesson1}---30 solid
circles, 15 striped circles, 10 solid triangles, and 5 striped
triangles---are to be modeled with the two indicator features
$f_{\textrm{circle}}$ and $f_{\textrm{solid}}$. With $\vec{\theta} =
0$ we have the uniform distribution, so the solid circle is contained
in its gray outline ($\empirical{}(\textrm{solid circle}) >
p_{\vec{\theta}}(\textrm{solid circle})$), the striped triangle
contains its gray outline ($\empirical{}(\textrm{striped triangle}) <
p_{\vec{\theta}}(\textrm{striped triangle})$), and the striped
circle and gray outline are coincident ($\empirical{}(\textrm{striped
  circle}) = p_{\vec{\theta}}(\textrm{striped circle})$).

A student can try various activities:

In the \textit{outcome matching activity}, the goal is to match the
model $p_{\vec{\theta}}$\ to $\tilde{p}$.
%--- find $\vec{\theta}$ such that $p_{\vec{\theta}}(y\mid x) 
%= \tilde{p}(y\mid x)$, for all $(x,y)$. 
The game is to make all of the outcomes match their corresponding
gray outlines in size (and color).  The student ``wins'' once the
maximum number of objects turn gray.
% DOESN'T MAKE SENSE: JUST IGNORE REGULARIZATION FOR NOW
% taking into account any regularization penalty.

In the \textit{feature matching activity}, the goal is to match the
expected feature vector $\mathbb{E}_{p_{\vec{\theta}}}[\vec{f}]$ to
the observed feature vector $\mathbb{E}_{\tilde{p}}[\vec{f}]$.  In
Figure \ref{fig:lesson1}, the student would seek a model that
correctly predicts the total number of circles and the total number of
solid objects---even if the specific number of solid circles is
predicted wrong.  (The predicted and observed counts for a {\em
  feature} can easily be found by adding up the displayed counts of
individual {\em outcomes} having that feature.  For convenience, they
are also displayed in a tooltip on the feature's slider.)  This game
can always be won, even if the given features are not adequately
expressive to succeed at outcome matching on the given dataset.

In the \textit{log-likelihood activity}, the goal is to maximize the
log-likelihood.  The log-likelihood bar (Figure~\ref{fig:lesson1})
adapts to changes in $\vec{\theta}$, just like the shapes. The game is
to make the bar as long as possible.\footnote{Once the gradient is
  introduced in a later lesson, knowing when you have ``won'' becomes
  clearer.}
%As shown in \NoteFF{fix} Figure \ref{fig:llbar}, we also represent the regularization penalty
%, and the objective under the ``true'' (generating) model parameters, when known.
In later lessons, the student instead tries to maximize a {\em regularized}
version of the log-likelihood bar, which is visibly shortened by a penalty for 
large weights (to prevent overfitting).

\begin{figure}[t]
\centering
\small
\begin{tabular}{
>{\centering\arraybackslash}m{.18\columnwidth} 
>{\centering\arraybackslash}m{.18\columnwidth}
>{\centering\arraybackslash}m{.20\columnwidth}
>{\centering\arraybackslash}m{.18\columnwidth}}

\multirow{2}{ .18\columnwidth }{\textbf{Quantity of Interest}} 
& $\mathbf{>0}$ 
& $\mathbf{= 0}$ & $\mathbf{<0}$\\ 
& {\bf \color{red} \texttransparent{.55}{ red } }
& {\bf \color{gray}\texttransparent{.65}{ gray } }
& {\bf \color{blue} \texttransparent{.6}{ blue } }\\  \\ \\
%\vspace{.75em}
$\empirical{} -p_{\vec{\theta}}$& \includegraphics[scale=.25]{images/goldilocks-circle-small.PNG}
& \includegraphics[scale=.25]{images/goldilocks-circle-justright.PNG}
& \includegraphics[scale=.25]{images/goldilocks-circle-large.PNG}\\ \\

$\mathbb{E}_{\empirical{}}\left[\cdot\right] 
- \mathbb{E}_{{p_{\vec{\theta}}}}\left[\cdot\right]$
%$\mathbb{E}_{\empirical{}}\left[\vec{f}\right] 
%- \mathbb{E}_{{p_{\vec{\theta}}}}\left[\vec{f}\right]$
%, and $\nabla_{\vec{\theta}} F$
& {\bf \color{red} \texttransparent{.55}{ 15 } }
& {\bf \color{gray}\texttransparent{.65}{ 30 } }
& {\bf \color{blue} \texttransparent{.6}{ 60 } }\\  \\
\vspace{.5em}

$\nabla_{\vec{\theta}} F$ 
& \includegraphics[scale=.25]{images/goldilocks-gradient-small.PNG}
& \includegraphics[scale=.25]{images/goldilocks-gradient-justright.PNG}
& \includegraphics[scale=.25]{images/goldilocks-gradient-large.PNG}\\ \\ 

\end{tabular}
\caption{Color and area indicate differences betwen the empirical
  distribution (gray outline) and model distribution. Red (or blue)
  indicates a model probability or parameter that should be
  increased (or decreased) to fit the data.}
\label{fig:colorsize_inventory}
\end{figure}

\begin{figure}[t]
\centering
\small
\includegraphics[scale=.65]{images/gradient-lesson7.PNG}
\caption{Gradient components use the same color coding as given in
  Figure \ref{fig:colorsize_inventory}. The length of each component
  indicates its potential effect on the objective.  Note that the
  sliders use a nonlinear scale from $-\infty$ to $+\infty$.}
\label{fig:gradients}
\end{figure}

Winning any of these games with more complex models becomes difficult
or at least tedious, so automatic methods come as a relief.  The
student may view {\em hints} on the sliders, showing which way each
slider should be nudged (Figure~\ref{fig:gradients}).  These hints
correspond to components of the log-likelihood gradient.  Further
automation is offered by the ``Step'' button, which automatically
nudges all parameters by taking a step of gradient
ascent,\footnote{When $\ell_1$ regularization is used, the optimal
  $\vec{\theta}$ often contains many 0 weights, and a step
  is not permitted to jump over a (possibly optimal) weight of
  0.\NoteJE{Is this needed for correctness or just efficiency?  I
    can't think anymore why nondifferentiability is a problem.  Isn't
    Armijo enough?}  
  It stops at 0, though if warranted, it can continue past 0 on the next step.} 
and even more by the ``Solve''
button, which steps all the way to the maximum.\footnote{The ``Solve''
  button adapts the stepsize at each step, using a backtracking line
  search with the Armijo condition.  This ensures convergence.}

Our lessons guide the student to appreciate the relationship
among the three activities.  First, feature matching is a weaker,
attainable version of outcome matching (when outcome matching is
possible it certainly achieves feature matching as well).  Second,
feature matching is equivalent to maximizing the (unregularized)
log-likelihood.  Thus the mismatch is 0 iff the gradient of
log-likelihood is 0.  In fact, the mismatch equals the gradient even
when they are {\em not} 0!  Thus, dragging the sliders in the
direction of the gradient hints can be viewed as a correct strategy
for {\em either} the feature matching game {\em or} the log-likelihood
game.  This connection shows that the current gradient of log-likelihood can
easily be computed by summing up the observed and currently predicted
counts of each feature.  After understanding this and playing with the ``Step'' and
``Solve'' buttons, the student should be able to imagine writing code
to train log-linear models.

% OLD 
% These games are not played in isolation. When playing either the log-likelihood or 
% feature matching games, we encourage the student to think how the two games are related: what 
% does each game try to accomplish, and how does it differ from what the other game achieves?
% % This is particularly true as more advanced concepts such as regularization and 
% % conjoined features are introduced.

\subsection{Guided Exploration}

% TRUE FOR ALL MANIPULATIVES
% A main tenet of this manipulative is the benefit of ``learning by playing.'' 
We expect students to ``learn by playing.''  The user can experiment
at any time with the sliders, with gradient ascent and its stepsize,
with the type and strength of regularization, and with the size of the
dataset.  The user can also sample new data or new parameters, and can
peek at the true parameters.  These options are described further
in Section \ref{sec:lessons}.

% The instructions and auxiliary material can help a student explore, but sometimes the 
% student may just need to roam freely. 

We encourage experimentation by providing \textit{tooltips} that
appear whenever a student hovers the mouse pointer over a element of
the GUI.  Tooltips provide guidance about whatever the student is
looking at \textit{right then}.  Some are static explanations (e.g.,
what does this gray bar represent?).  Others dynamically update with
changes to the parameters (e.g., the tooltips on the feature sliders
show the observed and expected counts of that feature).

Students see the tooltips repeatedly, which can help them absorb and
reinforce concepts over an extended period of time.  Students who like
to learn by browsing and experimenting can point to various tooltips
and get a sense of how the different concepts fit together.  Some
tooltips explicitly refer to one another, linking GUI elements such as
the training objective, the regularization choices, and the gradient.

% This allows those so inclined to
% go beyond the current lesson and make conceptual connections.

Though the user is welcome to play, we also provide some guidance.
Each lesson displays instructions that explain the current dataset,
justify modeling choices, introduce new functionality, lead the user
through a few activities, and ask lesson-specific questions.  The
first lesson also links to a handout with a more formal textbook-style
treatment.  The last lesson links to further reading and exercises.

% For more technical explanations and
% derivations, or for additional practice questions, they may direct the
% student to auxiliary material provided with the code; it is designed
% to explain the physical intuitions gained and act as a self-check on
% one's understanding.


\subsection{Instructor Interface: Creating and Tailoring Lessons}\label{sec:tailoring}

An instructor may optionally wish to tailor lessons to his or her
students' needs, interests, and abilities.  Shapes provide a nice
introduction to log-linear models, but eventually NLP students will
want to think about NLP problems, whereas vision students will want to
think about vision problems.  Thus, we have designed the manipulative
to handle text and arbitrary images, as well as the 12 shape-fill
combinations shown in Figure \ref{fig:shape_inventory}.

\begin{figure}[t]
\begin{center}
\centering
\includegraphics[scale=.5]{images/different_shapes_fills3x4-allgray.PNG}
\caption{Inventory of available shapes
  (circle/triangle/square/pentagon) and fills (solid/striped/hollow).
  %Color is reserved to indicate the sign of \eqref{eqn:obsexp}.  
 Color is reserved to indicate differences of expectations, e.g., 
 the sign of \eqref{eqn:obsexp}---see Figure \ref{fig:colorsize_inventory}.
  Text  and arbitrary images may be used instead of shapes.}
\label{fig:shape_inventory}
\label{fig:inventory}
\end{center}
\end{figure}

Tailoring lessons to the students' needs is as simple as editing a
couple of text files. These must specify
\begin{inparaenum}[(1)]
\item a set of features, 
\item a set of contexts,\footnote{The set of contexts may be omitted
    when there is only one context (i.e., an unconditioned model).}
  and
\item for each context, a set of 
%\NoteJE{what does this mean?} 
%\NoteFF{no two outcomes can have the same feature vector. This is an unfortunate consequence of an early design decision that considered the data to be featurized type occurrences. Fixing it shouldn't be *that* difficult... I'll remove the word uniquely}
featurized events, including counts and visual positions.
\end{inparaenum}
This simple format allows one to describe some rather involved
models.  Some of the features may be ``hidden'' from the student, thereby allowing the student to experience 
model mismatch.  Note that the visual positioning information is pedagogically 
important: aligning objects by orthogonal descriptions can make feature contrasts stand out more, 
e.g., circles vs.\@ triangles or solid vs.\@ striped.

The configuration files can turn off certain features on a per-lesson basis (without programming).  This is 
useful for, e.g., hiding the ``Solve'' button in early lessons, adding new tooltips, or specializing
the existing tooltips on a per-lesson basis.

However, being a manipulative rather than a tutoring system, our
software does not monitor the user's progress through a lesson and
provide guidance via lesson-specific hints, warnings, questions, or
feedback.  (The software is open-source, so others are free to extend
it in this way.)

% OLD
% \footnote{An interesting extension would be to provide
%   even more specific hints that depend on the user's behavior
% It is theoretically possible to provide arbitrarily targeted 
% feedback, such as hints that depend on $\vec{\theta}$ and observed data counts. Code included in a particular lesson's instructions 
% may suffice for one-off explanations but a more general solution should consider extending the back-end 
% (Section \ref{sec:backend}). As our focus was on developing a general, wide-reaching tool, and such state-dependent 
% hints would have impeded our goal, the manipulative currently has neither functionality. Under its open-source terms, 
% instructors are free to extend it.}

\subsection{Back-End Implementation}\label{sec:backend}
Anyone can use our virtual manipulative simply by visiting its website.
% designed to be widely available and have no start-up cost. 
There is no start-up cost.
Aside from reading the data, model and instructions from the web server, it is fully 
client-side. The Javascript back-end uses common and well-supported open-source 
libraries 
% for cross-browser compatibility and visualization.
that provide a consistent experience across browsers.\footnote{Specifically and in order, 
\texttt{d3} (\url{d3js.org/}),
\texttt{jQuery} (\url{jquery.com/}), 
\texttt{jQuery UI} (\url{jqueryui.com}),
\texttt{jQuery Tools} (\url{jquerytools.org/}), and
\texttt{qTip} (\url{craigsworks.com/projects/qtip/}).}
The manipulative relies on certain capabilities from the HTML5
standard.  Not all browsers in current use support these
capabilities, notably Internet Explorer 9 and under.
%\NoteJE{which browsers don't?}  
%\NoteFF{Internet Explorer; annoyingly, IE $< 10$ doesn't work though
%most users who use IE fall into that category.}
The tool works with recent versions of Firefox, Chrome and Safari.
%as well as Internet Explorer versions $\geq 10$.\NoteJE{you've tested
%  it with version 10, yes?  I can't be positive from your comments
%  below.  But we need to make a precise statement about IE
%  so that instructors can warn their students not to use IE 9.  I just
%  tried it on IE 10: the True LL bar appears everywhere, but if that
%  bug can be fixed, it looks ok.}
% and our students did not have trouble running it on them.
%\NoteJE{true?}
%\NoteFF{I don't have numbers to back it up, but generally I'd say yes.
%A couple people had issues with earlier versions of Firefox, but that was FF $< 11$. 
%Once I told them to upgrade it worked.
%
%More strongly, I can say that I've tested it in recent versions of the above
%three browsers. Some work better than others (e.g., Chrome is ``smoother'' 
%than FF), but nothing is broken.}

% so we do not believe this restriction to be unreasonable, 
% especially as all browers adopt widely used standards.

%The manipulative attempts to be responsive to an individual user's browser and display options, and 
%heuristically make efficient use of available space. That being said, higher resolution/larger displays 
%are desirable when dealing with the full model and some of the larger datasets introduced in later 
%lessons (see Section \ref{sec:conditionallessons}).
%
%mention that some ui considerations can result in very slight rounding errors
%
%solver: gradient ascent is simple (except for $\ell_1$ regularization), but it can take some finessing to make it visually appealing. We recursively call the solve function on a polynomial-decreasing schedule
%
%sigmoidal scaling for sliders, log-likelihood bars

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pedagogical Aims}\label{sec:aims}

\subsection{Modeling and Estimation}

When faced with a dataset $\Data{}$ of $(x, y)$ pairs, one often hopes
to \textbf{choose an appropriate model}.  When are log-linear
models appropriate?
% contrast with the empirical distribution \eqref{eqn:empirical_distr}.  While pragmatically the most important
% consequence of this modeling decision is that of proper feature design, we can ask about 
Why does their hypothesis space include the uniform distribution?  For
what feature sets does it include {\em every} distribution?  

One should also understand statistical \textbf{estimation}.  How do
the features interact?  When estimating their weights, can raising one
weight alter or reverse the desired changes to other weights?  How can
parameter estimation go wrong statistically (overfitting, perhaps
driving parameters to $\pm\infty$)?  What might happen if we have a
very large feature set?  Can we design regularized estimators that
prevent overfitting (the bias-variance tradeoff)?  What is the effect
of the regularization constant on small and large datasets?  On rare
and frequent contexts?  On rare and frequent features?  On useful
features (including features that always or never fire) and useless
ones?

% What estimators exist and how ``good'' are they? How does
% \eqref{eqn:loglin} generalize other distributions: $\vec{\theta} = 0$
% yields the uniform distribution, but can you match the empirical? What
% constraints does the model satisfy and what practical implications are
% there?

%How does this relate to the maximum entropy view?
% (if any; eg., maxent formulation or empirical
%MLE?)

Finally, one is responsible for \textbf{feature design}.  Which
features usefully distinguish among the events?  How do non-binary
features work and when are they appropriate?  When can a feature
safely be omitted because it provides no additional modeling power?
How does the choice of features affect generalization, particularly if
the objective is regularized?
%
% TOO SPECIFIC FOR THIS SECTION; NOW SUMMARIZED ABOVE
% Say we define a globally normalized model over solid circles, striped
% circles, solid triangles and striped triangles.  We might first
% consider the contrasting indicator feature functions for whether a
% shape is a circle or is solid, as in Figure \ref{fig:lesson1}. Which
% of the games can the student ``win'' using this two feature model? How
% might the empirical distribution affect the result?  Will more
% features always help? Why might partitioning binary features be
% redundant in some cases (definining both circle and triangle features
% for the above example), yet insufficient in others?
% %\footnote{Those with knowledge of statistics might benefit 
% %from a discussion of the importance of rank in exponential families.}
% 
% DOESN'T ADD ANY CONTENT
% We contrast conditional models and globally normalized ones. 
% These context-unaware models allow students to focus on
% understanding some of the core underlying mathematical concepts of
% exponential models, such as feature interations and weight tradeoffs,
% and important differences between global and conditional models. 
In particular, how do shared features and backoff features allow a model to
generalize to novel contexts and outcomes (or rare ones)?  How do the
resulting patterns of generalization relate qualitatively to
traditional smoothing techniques in NLP \cite{chen-goodman-1996}?
% REPHRASED ABOVE
% For instance, a benefit of conditional models is they can share 
% features across conditions, which helps in generalizing to unseen context-outcome 
% pairs and maximizing the regularized objective.

% REDUNDANT
% The empirical distribution is concerned with \textit{type} counts, but log-linear models 
% deal with descriptions of types; thus we can instead talk about \textbf{feature counts}. Our 
% manipulative allows exploration of these two different counts and how they affect the 
% feature weights. For example, if a feature is predicted to occur less often than it actually does, 
% we should raise its weight. But raising that weight 
% may affect the quality of all the other weights, so fitting one weight in
% isolation may reduce or reverse the need to raise other weights. How can you mitigate  
% adverse effects and get to the solution quickly, even in complex models?

% MENTIONED ABOVE
% Generally, both the number and range of features affect the expressive 
% power of the model, as with enough features you can fit any distribution. But this expressiveness 
% has a cost, as too many features can lead to overfitting. 
% MENTIONED ABOVE
% In one particularly important case, a feature for each $(x,y)$ pair allows an unregularized model to fit the 
% data perfectly. 
% Similarly, defining features that everything (or nothing) has can make the weights 
% take extreme values (go to $\pm \infty$). 
% I DON'T KNOW, AND OUR LESSONS DON'T COVER IT
% Further, though features are commonly binary, 
% this is not a strict requirement: defining non-binary feature functions, such as one that fires in 
% proportion to some property of the datum (e.g., the number of sides of a shape) can affect the
% proportions of feature counts. 
% How does this interact with the amount of training data and what is its 
% effect on model quality?

% ALL COVERED ABOVE NOW
% Our manipulative must generalize the above points to conditional models. Students should 
% understand the effect the amount of data (per context) has on what features are appropriate to define. 
% In particular, how do the marginal counts affect the resulting distributions, feature 
% weights and the results of the matching and log-likelihood games? 
% While exploring what backoff features are, and how and why they are useful, is important in its own right, 
% it is important to connect them to smoothing, a topic with which some students may have previous experience. 
% Understanding which features influence 
% what conditional distributions, e.g., that features defined on conditions have no effect on conditional distributions, 
% helps unite previously introduced concepts.% such as feature definition and model expressiveness considerations.

% defined over \Data{} , we are interested in estimating distributions 
%\begin{equation}
%p_{\vec{\theta}}\left(y\ \mid\ x\right) = \frac{u(x, y)}{\sum_{y'} u(x,y')},
%\label{eqn:conditional_loglin}
%\end{equation}
%from the unnormalized scores $u(x,y)$
%\begin{eqnarray}
%u(x,y) & = & \exp{\left(\vec{\theta}\cdot \vec{f}(x,y)\right)}.%\\
%%& = & \exp{\left(\sum_{k=1}^K \theta_k f_k(x,y)\right)}.
%\end{eqnarray}

%Therefore, we also 

\subsection{Training Algorithm} 

We also aim to convey intuitions about a specific training algorithm.
We use the regularized conditional log-likelihood \eqref{eqn:reg_ll}
to define the goodness of a parameter vector $\vec{\theta}$.
The best choice is then the $\vec{\theta}$ that solves equation \eqref{eqn:grad}:
\begin{equation}
\begin{aligned}
%\begin{split}
0 = \nabla_{\vec{\theta}} F
 = &
\ \mathbb{E}_{\empirical{}}\left[\vec{f}(X,Y)\right] 
- \mathbb{E}_{{p_{\vec{\theta}}}}\left[\vec{f}(X,Y)\right]\\
 & - C \nabla_{\vec{\theta}}R(\vec{\theta})
\label{eqn:grad} 
%= &\ \mathcal{C}(\empirical{}, p_{\vec{\theta}}) - C \nabla_{\vec{\theta}} R(\vec{\theta})\\
%\end{split}
\end{aligned}
\end{equation}
where because our model is conditional, $p_{\vec{\theta}}(x,y)$ denotes the hybrid distribution
$\empirical{}(x) \cdot p_{\vec{\theta}}(y\mid x)$.

Many important concepts are visible in \eqref{eqn:reg_ll} and \eqref{eqn:grad}.
% Some are new while others provide an alternative view
% on concepts already introduced. 
% %We highlight these connections, e.g., through 
% %tooltips and consistent color semantics, when possible.
%
As discussed earlier, \eqref{eqn:grad} includes the \textbf{difference between observed and
  expected feature counts}, 
\begin{equation}
\ \mathbb{E}_{\empirical{}}\left[\vec{f}(X,Y)\right] 
- \mathbb{E}_{p_{\vec{\theta}}}\left[\vec{f}(X,Y)\right].
\label{eqn:obsexp} 
\end{equation}
Students must internalize this concept and the meaning of the two counts above.
This prepares them to understand the extension to structured prediction,
where these counts can be more difficult to compute (see Section
\ref{sec:history}).  It also prepares them to generalize to training
latent-variable models \cite{petrov-klein-2008}.  In that setting, the
observed count can no longer be observed but is replaced by another
expectation under the model, conditioned on the partial training data.

% This difference of expectations, present in the feature matching activity and consideration of expectation 
% proportions, is a recurring quantity. We encourage students to infer the feature 
% matching and log-likelihood activities are really just the same thing, and focusing on this difference as part 
% of a larger exploration into the gradient confirms the physical intuitions with mathematical justification.
% This difference also shows that, when outcome matching is feasible, matching the empirical 
% distribution will generally maximize the (unregularized) log-likelihood. Of course, overfitting is possible and 
% may result in a larger log-likelihood than the true generating distribution yields.

\eqref{eqn:grad} also includes a \textbf{weight decay} term for
regularization.  We allow both $\ell_1$ and $\ell_2$ regularization:
$R(\vec{\theta}) = \|\vec\theta\|_1$ versus $R(\vec{\theta}) =
\|\vec{\theta}\|_2^2$.  One can see experimentally that strong $\ell_1$
regularization tries to use a few larger weights and leave the rest at
0, while strong $\ell_2$ regularization tries to share the work among
many smaller weights.  One can observe how for a given $C$, the
regularization term is more important for small datasets, since for larger
datasets it is dominated by the log-likelihood.

% NOW MENTIONED EARLIER
% \footnote{Our  optimizer special-cases the non-differentiable
% $\ell_1$ regularization, 
%
% NO WE DON'T, AS FAR AS I CAN SEE!
% but we inform the students of this difficulty.}

Once one can compute the gradient, one can ``follow'' it along the
surface, in a way that is guaranteed to increase the convex objective
function up to its global maximum.  The ``Solve'' button does this and
indeed one can watch the log-likelihood bar continually increase.  Yet
one should observe what might go wrong here as well.  Gradient
ascent can oscillate if a {\em fixed} stepsize is used (by clicking
``Step'' repeatedly).  One may also notice that ``Solve'' is somewhat
slow to converge on some problems, which motivates considering
alternative optimization algorithms \cite{malouf2002comparison}.

% if the current weights are
% strongly coupled (the Hessian is ill-conditioned) 

% \footnote{We focus on
%   gradient ascent but other parameter estimation methods exist, such
%   as variants of iterative scaling, and first- and second-order
%   gradient algorithms. The most popular methods have relied on
%   quantities similar to \eqref{eqn:obsexp}, which, as shown in Figure
%   \ref{fig:colorsize_inventory} and discussed above
%   (\S\ref{sec:aims}), is one of the most important concepts we
%   illustrate. See \newcite{berger-dellapietra-dellapietra-1996} and
%   \newcite{malouf2002comparison}. }
%
%Convexity explains these two facts.\footnote{Advanced 
%students might try ``breaking'' convexity, e.g., what happens when $C
%< 0$?}

% NAH, THERE AIN'T NO HILL IN SIGHT
% We find the canonical landscape-inspired convexity metaphor useful, and we partially reflect 
% the metaphor in the the adaptable log-likelihood bar. Rather than just \textit{imagining} walking up a hill, 
% students can ``actually'' do it.

% REDUNDANT
% We use the gradient to further illustrate feature modeling considerations. The most obvious is to show how 
% minimizing a single isolated $|\frac{\partial}{\partial \theta_k} F|$ might adversely affect another partial 
% $\frac{\partial}{\partial \theta_{k'}}F$. Additional connections to differences in type and feature counts should be 
% made: even if the model distribution does not fully match the empirical, expected and observed feature 
% counts should match.

% REDUNDANT
% Underlying the above concepts is \textbf{general machine learning} and 
% \textbf{statistical knowledge}. These include what estimators are,  how different data or model configurations 
% affect their quality, and how to limit adverse or prevent lower quality estimators. For example, 
% a small sample size leads to estimators with higher variance; regularization is a way to mitigate that, albeit at the expense of 
% the training log-likelihood. 
% Similarly, why can regularization be considered a form of smoothing that fits the data 
% imperfectly? The model might have fewer parameters than 
% outcomes, or it might forgo many parameters to avoid being penalized by 
% the regularizer.  Further, to limit regularization penalty, the model can make 
% use of backoff features and not just fine-grained features, thereby resulting 
% in \textit{backoff smoothing}. These concepts are connected to the expressiveness 
% considerations introduced in feature modeling.

We should note that we are \textit{not} concerned with efficiency issues, e.g.,  
tractably computing the normalizers $Z(x)$. Efficient normalization
is a crucial practical ingredient in {\em using} log-linear models,
but our primary concern is to impart a near-physical intuitive
understanding of the models themselves. See Section \ref{sec:history}
or \newcite{smith-2011} for strategies on computing the normalizer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Provided Lessons}\label{sec:lessons}
In this section we provide an overview of the \NumLessons{} currently
available lessons.  (Of course, you can work through the lessons
yourself for further details.)  ``Core'' lessons that build intuition
precede the ``applied'' lessons focused on NLP tasks or problems.
Instructors should feel especially free to replace or reorder the
``applied'' lessons.

Core lessons \les{1}--\les{5} provide a basic
\textbf{introduction} to log-linear modeling, using unconditioned
distributions over only four shapes as shown in Figure
\ref{fig:lesson1}.  We begin by matching outcomes using just
``circle'' and ``solid'' features.  We discover in lesson \les{2}
that it is redundant to add ``triangle'' and ``striped'' features.  In
lesson \les{3} we encounter a dataset which these features cannot
fit, because the shape and fill attributes are not statistically
independent.  We remedy this in lesson \les{4} with a conjunctive
``striped triangle'' feature.  

Because outcome matching fails in lesson \les{3}, lessons
\les{3}--\les{4} introduce feature matching and log-likelihood
as suitable alternatives.  Lesson \les{5} briefly illustrates a
non-binary feature function, ``number of sides'' (taking values 3, 4,
and 5 on triangles, squares, and pentagons).  This clarifies the
matching of feature counts: here we are trying to predict the total
number of sides in the dataset.

% \footnote{Students had trouble with non-binary features, so
%   we improved our explanation of expected and observed counts
%  \eqref{eqn:obsexp} in this context.}

% We keep 
% the models simple and incrementally build intution, and only consider joint models over shapes. The focus is 
% on feature design. We start by comparing models over four shapes with two and four features. 
% Next we hide certain generating features from the students: they 
% consider what is going on in this model mismatch exercise, and a possible way to handle it. We play all three games, 
% starting with model matching but progressing to feature matching and the log-likelihood game; we compare and 
% constrast these latter two to show that they are really just doing the same thing. Students explore 
% non-binary features too.

% Having begun to develop the appropriate intuition, 
Lessons \les{6}--\les{8} focus on \textbf{optimization}.  They
move up to the harder setting of 9 shapes with 6 features, so we tell
students how to turn on the gradient ``hints'' on the sliders.  We
explain how these hints relate to feature matching and log-likelihood.
We invite the students to try using the hints on earlier lessons---and
on new random datasets that they can generate by clicking.  In Lesson
\les{7}, we introduce the ``Step'' and ``Solve'' buttons to help
even more with a difficult dataset.  Students use all these GUI
elements to climb the convex objective and increase the log-likelihood
bar.

At this point we introduce \textbf{regularization}.  Lesson \les{6}
invited students to generate {\em small} random datasets and observe
their high variance and the tendency to overfit them.  Lesson
\les{8} gives a more dramatic illustration of overfitting: with no
observed pentagons, the solver sends $\theta_{\textrm{pentagon}}
\rightarrow -\infty$ to make $p_{\vec{\theta}}(\textrm{pentagon})
\rightarrow 0$.  We prevent this by adding a regularization penalty,
which reserves some probability for pentagons.  Striped pentagons turn out
to be the least likely pentagons, because stripes were observed to be
uncommon on {\em other} shapes (so $\theta_{\textrm{striped}} < 0$).
Thus we see that our choice of features allows this ``smoothing
method'' to make useful generalizations about novel outcomes.

% additional outcomes, and as a result, larger models. To deal with this complexity, we describe the convex 
% objective and its gradient 
% (Figure \ref{fig:gradients}), 
% automatically solving for $\vec{\theta}$ via gradient ascent, and how a larger partial derivative means the objective is more 
% sensitive to changing that feature. We connect the gradient, specifically computing observed and expected feature 
% counts, with the previous lessons involving model mismatch; this allows students to better understand what the 
% model was learning and why. 
% Students can use the gradient hints and solver to watch $\vec{\theta}$ climb up the objective ``hill'' to the global maximum, 
% partly illustrating the implications of a convex objective for our model's estimator.
 
% We also introduce additional manipulative functionality and encourage their use to 
% explore log-linear models. For instance, students can generate secret oracle weights and 
% fit a model to data sampled from those new weights. 

% Students can also ``cheat'' and look at the true 
% weights,  and question why estimates of $\vec{\theta}$ might be different from the true parameters 
% (sampling error).

Lessons \les{9}--\les{10} consider the effect of $\ell_1$ versus
$\ell_2$ regularization, and the competition between the regularizer
(scaled by the constant $C$) and the log-likelihood (scaled by the
dataset size $N$).\footnote{Clever students may think to try setting
  $C < 0$, which breaks convexity of the objective function.}

% modeling. Students explore what affects model quality, such as which regularizer is used, the 
% regularization coefficient, and the amount of \textit{training} data.\footnote{Advanced 
% students might try ``breaking'' convexity, e.g., by setting $C < 0$.} How does regularization help with 
% unseen (``test'') data? 

Lessons \les{11}--\les{13} introduce \textbf{conditional models},
showing how features are shared among three contexts.  The third
context is unobserved, yet our trained model makes plausible
predictions about it.  The {\em conditional} probabilities of
unobserved shapes are positive even without regularization, in
contrast to the {\em joint} probabilities in lesson \les{9}.

We see that a frequent context $x$ generally has more influence on the
parameters.  But this need not be true if the parameters do not help
to distinguish among the particular outcomes ${\cal Y}(x)$.

% conditional and global models. Students use a conditional model with shared features to model 
% heavily skewed data, which illustrates how the matching and log-likelihood games differ under the two types of 
% models. We illustrate how shared conditional features can help more accurately model a context-specific 
% outcome $\mathcal{Y}(x)$.

Lessons \les{14}--\les{15} explore feature design in conditional
models.  We model conditional probabilities of the form $p(\text{fill}
\mid \text{shape})$.  ``Unigram'' features can favor certain fills $y$
regardless of the shape.  ``Bigram'' features that look at $y$ and $x$
together can favor different fills for each shape type.  We see that
features that depend {\em only} on the shape $x$ cannot distinguish
among fills $y$, and so have no effect on the conditional
probabilities $p(y\mid x)$.

% Specifically in lesson \les{14}, we add a feature for each context and the students are asked what happens 
% when we solve and why; that is, why context-only features do not move from their initial 0 values. 

Lesson \les{15} illustrates how regularization promotes
generalization and feature selection.  Once we have a full set of
bigram features, the unigram features are redundant.  We never have to
put a high weight on ``solid'': we can accomplish the same thing by
putting high weights on ``solid triangle'' and ``solid circle''
separately.  Yet this misses a generalization because it does not
predict that ``solid'' is also likely for pentagons.  Fortunately,
regularization encourages us to avoid too many high weights.  So
we prefer to put a single high weight on ``solid,'' and use
the ``solid triangle'' and ``solid circle'' features only to model
smaller shape-specific deviations from that generalization.  As a
result, we will indeed extrapolate that pentagons tend to be
solid as well.

% In lesson \les{15}, we consider conjoined ``bigram'' features as a way to successfully incorporate 
% contextual information into the model. We retain the original ``unigram'' (outcome) features, and explore
% how the regularizer breaks parameter setting ties.

Lesson \les{16} begins the application-driven lessons:

One lesson builds on the ``unigram'' and ``bigram'' concepts to create
a ``bigram language model''---a model of shape {\em sequences} over a
vocabulary of 9 shapes.  A shape's probability depends not only on its
attributes but also on the attributes that it shares with the previous
shape.  What is the probability of a striped square given that the
previous shape was also striped, or a square, or a striped square?

% We
% step through the design of three general features that look at the
% event and the context together and ask open-ended questions that cause
% students to ponder estimation issues in conditional sequence models
% even when there is no model mismatch.
% %Even though the data were generated from the ``true'' model, how well does the model fit the observed data sample, and how well can the model recover the ``true'' weights? Should regularization be used to answer these questions? What should you do if you need a very good estimate of those weights?

We also apply log-linear modeling to the task of text categorization
(spam detection).  We challenge the students to puzzle out how this
model is set up and how to generalize it to three-way categorization.
Our contexts in this case are documents---actually very short phrases.
Most contexts are seen only once, with an outcome of either ``mail''
or ``spam.''  Our feature set implements logistic regression
(footnote~\ref{fn:logistic}): each feature conjoins 
$y=\textrm{spam}$ with some property of the text $x$, 
such as ``contains 'parents'{},'' ``has boldface,'' or ``mentions money.''

% , and so 
% we first ask some simpler questions to make sure they understand what is going on and what the conditional models actually mean.
% Then, we can ask particular questions and make sure they understand how the feature modeling experience gained in 
% prior lessons should be applied in practice. We also ask them to think about how to extend the approach to do three-way classification 
% of a phrase (e.g., as \texttt{spam}, \texttt{work}, or \texttt{fun}).
% 
% %What kind of information below indicates that a particular training phrase has been labeled as spam?
% %Which are the two test phrases included below? How do you know?
% %If you train this model without regularization, what happens to the log-likelihood and the weights? Why?
% %If you train this model with regularization, how does it classify the two test phrases? Look at the weights: what features helped it make this classification?
% %You might think that 'fortune' fires on any phrase that contains the word 'fortune'. But lesson 14 suggests that can't quite work. Let's be precise: what (context,outcome) pairs is 'fortune' really defined to fire on?
% %mentions money is a binary feature. What are some words that cause it to fire? How did you find out?
% %Is the feature 'yada' a binary feature, or a counting feature? That is, does fyada return 1 or 3 on yada yada yada? How did you find out?
% %Which of the non-spam messages does your trained model think is spammiest? How do you know?

Additional linguistic application lessons may be added in the near
future---e.g., modeling the relative probability of grammar rules or
parse trees.

The final lesson summarizes what has been learned, mentions connections
to other ideas in machine learning, and points the student to further
resources.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Graduating to Real Applications}\label{sec:history}

At the time of writing, 3266 papers in the ACL Anthology mention
log-linear models, with 137 using ``log-linear,'' ``maximum entropy''
or ``maxent'' in the paper title.  These cover a wide range of
applications that can be considered in lectures or homework projects.

Early papers may cover the most fundamental applications and the
clearest motivation.  Conditional log-linear models were first
popularized in computational linguistics by a group of researchers
associated with the IBM speech and language group, who called them
``maximum entropy models,'' after a principle that can be used to
motivate their form \cite{jaynes-1957}.  They applied the method to
various binary or multiclass classification problems in NLP, such as
prepositional phrase attachment \cite{ratnaparkhi-1994}, text
categorization \cite{nigam-lafferty-mccallum-1999}, and boundary
prediction \cite{beeferman-berger-lafferty-1999}.

Log-linear models can be also used for structured prediction problems
in NLP such as tagging, parsing, chunking, segmentation, and language
modeling.  A simple strategy is to reduce structured prediction to a
sequence of multiclass predictions, which can be individually made
with a conditional log-linear model \cite{ratnaparkhi-1998}.  A more
fully probabilistic approach---used in the original ``maximum
entropy'' papers---is to use \eqref{eqn:loglin} to define the
conditional probabilities of the steps in a generative process that
gradually produces the structure
\cite{rosenfeld-1994,berger-dellapietra-dellapietra-1996}.\footnote{Even
  predicting the single next word in a sentence can be broken down into a
  sequence of binary decisions in this way.  This avoids normalizing over the
  large vocabulary \cite{mnih-hinton-2008}.}  This idea
remains popular today and can be used to embed rich distributions into
a variety of generative models \cite{bergkirkpatrick-et-al-2010}.  For
example, a PCFG that uses richly annotated nonterminals involves a
large number of context-free rules.  Rather than estimating their
probabilities separately, or with traditional backoff smoothing, a
better approach is to use \eqref{eqn:loglin} to model the probability
of all rules given their left-hand sides, based on features that
consider attributes of the nonterminals.\footnote{E.g., case, number,
  gender, tense, aspect, mood, lexical head.  In the case of a
  terminal rule, the spelling or morphology of the terminal symbol can
  be considered.}

The most direct approach to structured prediction is to simply predict
the structured output all at once, so that $y$ is a large structured
object with many features.  This is conceptually natural but means
that the normalizer $Z(x)$ involves summing over a large space ${\cal
  Y}(x)$ (footnote~\ref{fn:bigY}).  One can restrict ${\cal Y}(x)$
before training \cite{johnson-et-al-1999}.  More common is to sum {\em
  efficiently} by dynamic programming or sampling, as is typical in
linear-chain conditional random fields
\cite{lafferty-mccallum-pereira-2001}, whole-sentence language
modeling \cite{rosenfeld-chen-zhu-2001}, and CRF CFGs
\cite{finkel2008efficient}.  This topic is properly deferred until
such algorithmic techniques are introduced later in an NLP class, for
example in a unit on parsing (see discussion in
section~\ref{sec:whyloglin}).  We prepare students for it by
mentioning this point in our final lesson.\footnote{This material can
  also be connected to other topics in machine learning.  Dynamic
  programming and sampling are also used for exact or approximate
  computation of normalizers in undirected graphical models (Markov
  random fields or conditional random fields), which are really just
  log-linear models for structured prediction of tuples.}

% It might be valuable to add a dual representation and directly
% maximize the entropy subject to expectation constraints---see footnote
% \ref{fn:dual}.  To be general and remain focused on our pedagogical
% aims, the manipulative is limited in the complexity of the structured
% prediction it can currently handle. While one could change the backend
% to handle some of the solutions discussed above, students interested
% in structured prediction might find the manipulative more valuable if
% it includes examples that motivate those solutions.

% While many of the above applications present interesting models, some may be 
% too ambitious for class assignments, especially if students are seeing log-linear 
% modeling for the first time. 

Our final lesson also leads to a web page where we link to log-linear
software and to various pencil-and-paper problems, homework projects,
and readings that an instructor may consider assigning.  We welcome
suggested additions to this page.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
We have introduced an open-source, web-based virtual manipulative for
log-linear models. Included with the code are \NumLessons{} lessons
peppered with questions, a handout that gives a formal treatment of
the necessary derivations, and auxiliary information including
further reading, practice problems, and recommended software.  A
version is available at \WhereToFind{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\paragraph*{Acknowledgements}
We would like to thank the anonymous reviewers for their helpful feedback and suggestions 
and the entire Fall 2012 Natural Language Processing course at 
Johns Hopkins.


\bibliographystyle{acl}
\bibliography{tnlp}

\end{document}
