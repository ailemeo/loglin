FURTHER MATERIALS WILL GO HERE
(link back to main assignment)
(say that we welcome suggestions)

(add this to bin/ensure_permissions)
(link to it from near top of final lesson)

<H3>Log-Linear Software</H3>

(GIS/IIS, Malouf, Goodman, and other people with specialized training methods)

<H3>Pencil-and-Paper Exercises</H3>

(our practice problems, plus 3 from 2012)

<H3>Homework Projects</H3>

\NoteFF{NLTK says: ``Select one of the classification tasks described in this
  chapter, such as name gender detection, document classification,
  part-of-speech tagging, or dialog act classification. Using the same
  training and test data, and the same feature extractor, build three
  classifiers for the task: a decision tree, a naive Bayes classifier,
  and a Maximum Entropy classifier. Compare the performance of the
  three classifiers on your selected task. How do you think that your
  results might be different if you used a different feature
  extractor?'' Berkeley  maxent for proper noun classification, or POS
  tagging\protect{\url{http://www.cs.berkeley.edu/~klein/cs288/}}}

<H3>Further Reading</H3>

(list our own paper here)
(list our handout here)
(mention ACL anthology)

(@article{jaynes1982rationale,
  title={On the rationale of maximum-entropy methods},
  author={Jaynes, Edwin T},
  journal={Proceedings of the IEEE},
  volume={70},
  number={9},
  pages={939--952},
  year={1982},
  publisher={IEEE}
})


(@article{berger1996maximum,
  title={A maximum entropy approach to natural language processing},
  author={Berger, Adam L and Pietra, Vincent J Della and Pietra, Stephen A Della},
  journal={Computational linguistics},
  volume={22},
  number={1},
  pages={39--71},
  year={1996},
  publisher={MIT Press}
})

